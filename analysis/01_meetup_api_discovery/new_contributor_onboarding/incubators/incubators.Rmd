---
title: "Incubators"
author: "Augustina Ragwitz"
date: "July 27, 2017"
output: html_document
---

Scrape Seeddb.com for a list of incubators.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(dplyr)
library(rvest)
library(stringr)
library(tidyr)

# TODO make this a parameter
accelerators_html_file=paste("scraped_html/seeddb", "accelerators", "html", sep="_")
```

## Seedb Accelerators Page
```{r seedb_html, include=FALSE, echo=FALSE}

# Scrape the Accelerators Page and save the result as a local file
# Do not run this unless you want your local file to be overwritten!

download.file("http://www.seed-db.com/accelerators", accelerators_html_file)

```

## Parse Accelerators Table into Data Frame
```{r seeddb_accelerators_table, echo=FALSE}

seeddb_accelerators_html <- read_html(accelerators_html_file)

seeddb_accelerators <- seeddb_accelerators_html %>% 
  html_nodes("#accellist") %>%
  html_table(fill=TRUE) %>% as.data.frame()

seeddb_accelerators <- seeddb_accelerators %>% 
  mutate(Companies=X..Co.s, 
         Exits=X..Exits, 
         Funding=X..Funding,
         Average=Average..) %>%
  select(Program, Location, Companies, Exits, Funding, Average)

```

## Get Company List for Each Accelerator

```{r seeddb_accelerators_links, echo=FALSE}
seeddb_accelerators_links <- data.frame()
link_selector <- "#accellist > tbody > tr:nth-child($) > td:nth-child(1) > a:nth-child(1)"

# TODO use better tidyverse workflow for this!!
for(n in 1:nrow(seeddb_accelerators)) {
  selector <- str_replace_all(link_selector, "\\$", as.character(n))
  # extract link from html
  link_node <- seeddb_accelerators_html %>% 
    html_node(selector)
  # extract acc from html
  link <- link_node %>% html_attrs()
  link <- str_replace(link, "view", "viewall")
  acc <- link_node %>% html_text() 

  # merge with existing
  seeddb_accelerators_links <- bind_rows(
    seeddb_accelerators_links,
    data.frame(Program=acc, link=paste("http://www.seed-db.com", link, sep=""))
  )
}

seeddb_accelerators <- merge(seeddb_accelerators, seeddb_accelerators_links, by="Program")
rm(seeddb_accelerators_links)

write.csv(seeddb_accelerators, "seeddb_accelerators.csv", row.names = FALSE, na = "")

```

```{r}
seeddb_accelerators <- read.csv("seeddb_accelerators.csv")
seeddb_acc_has_companies <- seeddb_accelerators %>% filter(Companies > 0)
```

```{r seeddb_accelerators_fix_cities}

# not perfect but does the job
seeddb_accelerators_split <- seeddb_accelerators %>% 
  mutate(location_1 = strsplit(as.character(Location), " \\/|; ")) %>% 
  unnest(location_1) %>%
  separate(location_1, c("city", "location_2"), sep=", ", fill="right", extra="merge") %>%
  mutate(location_3=ifelse(is.na(location_2), city, NA)) %>%
  separate(location_2, c("state", "country"), sep=" ", fill="right", extra="merge") %>%
  separate(location_3, c("city2", "country2"), sep=" ", fill="right", extra="merge") %>%
  mutate(country=ifelse(is.na(country), country2, country),
         city=ifelse(is.na(state), city2, city))

seeddb_accelerators_usa <- seeddb_accelerators_split %>% 
  filter(country == 'US') %>% 
  select(-city2, -country2)

write.csv(seeddb_accelerators_usa, "seeddb_accelerators_usa.csv", row.names = FALSE, na = "")
```

```{r seeddb_accelerators_companies, include=FALSE}
# Create the archive of the HTML scrapes for the company pages
# Don't run this unless you want all of your archives to be overwritten

# TODO replace this with better titdyverse workflow
for(n in 1:nrow(seeddb_acc_has_companies)){
  row <- seeddb_acc_has_companies[n,]
  link <- as.character(row["link"])
  acc <- as.character(row["Program"])
  
  acc_filename <- str_replace_all(acc, "/| ", "_")
  acc_filename <- str_to_lower(acc_filename)
  acc_filename <- paste("scraped_html/", acc_filename, "companies", "html", sep="_")
  
  print(paste("getting HTML for", acc))
  seeddb_company_html <- download.file(link, acc_filename)
  print(paste("saved HTML to", acc_filename))
}

```

Crunchbase blocks bots. Angellist API doesn't provide location information about companies but could provide other insights for future research.

```{r seeddb_companies, echo=FALSE}
seeddb_companies <- data_frame()

# TODO: refactor so this isn't using for loops
for(n in 1:nrow(seeddb_acc_has_companies)){
  row <- seeddb_acc_has_companies[n,]
  acc <- as.character(row$Program)
  acc_filename <- str_replace_all(acc, "/| ", "_")
  acc_filename <- str_to_lower(acc_filename)
  acc_filename <- paste("scraped_html/", acc_filename, "companies", "html", sep="_")

  seeddb_company_html <- read_html(acc_filename)
  seeddb_company_table <- seeddb_company_html %>%
    html_nodes("#seedcos") %>%
    html_table(fill=TRUE) %>%
    as.data.frame()
  
  seeddb_company_table <- seeddb_company_table %>% 
    mutate(Company=Company.Name, 
         Website=Website...Crunchbase.links, 
         Exit=Exit.Value) %>%
    select(State, Company, Website, Cohort.Date, Exit, Funding)
  
  seeddb_company_table <- seeddb_company_table %>% 
    mutate(Program=acc)

  company_links <- data_frame()

  print(paste("Building Links for:", n, acc))
  for (i in 1:nrow(seeddb_company_table)) {
    crunchbase_link <- seeddb_company_html %>% 
      html_node(str_replace("#seedcos > tbody > tr:nth-child($) > td:nth-child(2) > a", 
                            "\\$", as.character(i))) %>% html_attrs()
    angellist_link <- seeddb_company_html %>% 
      html_node(
        str_replace("#seedcos > tbody > tr:nth-child($) > td:nth-child(3) > div > button:nth-child(1) > a",
                    "\\$", as.character(i))) %>% html_attrs()
    links_df <- data_frame(Company=c(seeddb_company_table[i,]["Company"]), 
                                  crunchbase=crunchbase_link[1], 
                                  angellist=angellist_link[1])
    links_df <- links_df %>% mutate(Company=as.character(Company))
    
    company_links <- bind_rows(company_links, links_df)
    
  }

  seeddb_company_table <- seeddb_company_table %>%
    inner_join(company_links, by="Company")
  
  seeddb_companies <- bind_rows(seeddb_companies, seeddb_company_table)
}

# Fix crunchbase urls
seeddb_companies <- seeddb_companies %>%
  mutate(crunchbase = ifelse(str_match(crunchbase, "^http") > 0, crunchbase, NA),
         angellist = ifelse(str_match(angellist, "angel.co") > 0, angellist, NA))

write.csv(seeddb_companies, "seeddb_companies.csv", row.names = FALSE, na = "")
```

```{r get_company_location}

# Currently doesn't work

seeddb_accelerators_usa <- read.csv("seeddb_accelerators_usa.csv", na=" ")
seeddb_acc_has_companies <- seeddb_accelerators_usa %>% filter(Companies > 0)

seeddb_companies <- read.csv("seeddb_companies.csv", na.strings= "")
seeddb_companies_lnk <- seeddb_companies %>% filter(!is.na(angellist))

seeddb_companies_subset <- seeddb_companies_lnk %>% 
  inner_join(seeddb_acc_has_companies %>% select(Program), by="Program")


for(n in 1:nrow(seeddb_companies_subset)){
  row <- seeddb_companies_subset[n,]
  row$Company <- levels(droplevels(row$Company))
  row$angellist <- levels(droplevels(row$angellist))
  company <- row$Company
  angellist_link <- row$angellist
  
  company_filename <- str_replace_all(company, "/| ", "_")
  company_filename <- str_to_lower(company_filename)
  company_filename <- paste("scraped_html/company", company_filename, "html", sep="_")
  
  print(paste("getting HTML for", n, company))
  seeddb_company_html <- tryCatch(download.file(angellist_link, company_filename),
                                  error=function (e) {
                                      print(paste(
                                        "Error downloading", angellist_link, "to", company_filename))
                                    },
                                  finally=print(paste(
                                    "Saving", angellist_link, "to", company_filename)))


  print(paste("saved HTML to", company_filename))
}

```

```{r}
# extract city from angel.co html

"#root > div.page.flush_bottom.dl85.layouts.fhr17.header._a._jm > div > div > div.prefix.u-bgWhite > div.u-bgWhite.dss98.startups-show.fhr17.header._a._jm > div > div > div > div > div > div > div.text.profile-text.u-textAlignCenterSmOnly > div > div.s-grid0-colMd24 > div > div.dssh27.startups-show-header_2.fts73.tags_and_links._a > div > span.js-location_tag_holder > span > span > a"
```

## Add Location Info to Incubators

Add the Metropolitan Areas to the cities identified in the Incubator data.

```{r}
incubators <- read.csv("seeddb_accelerators_usa.csv", stringsAsFactors = FALSE)
metro_areas <- read.csv("../tech_meetups/metro_areas_meetups.csv", 
                        na.strings = "", stringsAsFactors = FALSE)

# fix the incubators state abbreviations
incubators <- incubators %>% 
  mutate(state_alt=state.abb[match(state,state.name)],
         state=ifelse(is.na(state_alt), as.character(state), state_alt)) %>%
  select(-state_alt)

# fix a couple of bogus city names
incubators <- incubators %>%
  mutate(city=ifelse(city=="Greenville and Spartanburg", "Greenville", as.character(city)),
         city=ifelse(city=="Silicon Valley", "San Jose", as.character(city)))

# add "Ann Arbor" to metro areas
metro_areas[nrow(metro_areas) + 1, ] <- 
  c("C1982","MI","Detroit","Michigan","Detroit MI","MI","Ann Arbor","Detroit-Warren-Dearborn, MI MSA")
                         
# join on metro areas
incubators_msa <- incubators %>%
  left_join(metro_areas, by=c("city"="msa_cities", "state"="msa_states"))

write.csv(incubators_msa, "incubators_msa.csv", row.names = FALSE, na = "")

```


# Sources

[1] Angellist API https://market.mashape.com/community/angellist

